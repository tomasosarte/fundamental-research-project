{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035abcac-3d8c-4dd6-8722-a4e8abf2a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3047, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3102, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3489, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3549, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/40/y2m0v2h9587d4pnsspl6bs6c0000gn/T/ipykernel_18614/3295816518.py\", line 15, in <module>\n",
      "    from src.trainer.trainer import Trainer\n",
      "  File \"/Users/larabastos/Desktop/uni/msc/q4/frmdl/fundamental-research-project/experiments/../src/trainer/trainer.py\", line 2, in <module>\n",
      "    from torch.utils.tensorboard import SummaryWriter\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/torch/utils/tensorboard/__init__.py\", line 12, in <module>\n",
      "    from .writer import FileWriter, SummaryWriter\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py\", line 19, in <module>\n",
      "    from ._embedding import get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/torch/utils/tensorboard/_embedding.py\", line 10, in <module>\n",
      "    _HAS_GFILE_JOIN = hasattr(tf.io.gfile, \"join\")\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorboard/lazy.py\", line 65, in __getattr__\n",
      "    return getattr(load_once(self), attr_name)\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorboard/lazy.py\", line 97, in wrapper\n",
      "    cache[arg] = f(arg)\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorboard/lazy.py\", line 50, in load_once\n",
      "    module = load_fn()\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorboard/compat/__init__.py\", line 45, in tf\n",
      "    import tensorflow\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorflow/__init__.py\", line 55, in <module>\n",
      "    from tensorflow._api.v2 import compat\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorflow/_api/v2/compat/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.compat import v1\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorflow/_api/v2/compat/v1/__init__.py\", line 30, in <module>\n",
      "    from tensorflow._api.v2.compat.v1 import compat\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.compat.v1.compat import v1\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\", line 47, in <module>\n",
      "    from tensorflow._api.v2.compat.v1 import lite\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\", line 9, in <module>\n",
      "    from tensorflow._api.v2.compat.v1.lite import experimental\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.compat.v1.lite.experimental import authoring\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.lite.python.authoring.authoring import compatible # line: 263\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorflow/lite/python/authoring/authoring.py\", line 42, in <module>\n",
      "    from tensorflow.lite.python import convert\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorflow/lite/python/convert.py\", line 31, in <module>\n",
      "    from tensorflow.lite.python import util\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorflow/lite/python/util.py\", line 53, in <module>\n",
      "    from jax import jit as _jit\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/jax/__init__.py\", line 39, in <module>\n",
      "    from jax import config as _config_module\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/jax/config.py\", line 15, in <module>\n",
      "    from jax._src.config import config as _deprecated_config  # noqa: F401\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/jax/_src/config.py\", line 28, in <module>\n",
      "    from jax._src import lib\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/jax/_src/lib/__init__.py\", line 90, in <module>\n",
      "    import jaxlib.xla_client as xla_client\n",
      "  File \"/Users/larabastos/miniconda3/lib/python3.12/site-packages/jaxlib/xla_client.py\", line 29, in <module>\n",
      "    from . import xla_extension as _xla\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/tensorboard/compat/__init__.py:42\u001b[39m, in \u001b[36mtf\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorboard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m notf  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'notf' from 'tensorboard.compat' (/Users/larabastos/miniconda3/lib/python3.12/site-packages/tensorboard/compat/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Trainer\n",
    "from src.trainer.trainer import Trainer\n",
    "\n",
    "# Models\n",
    "from src.models.mlp import MLP\n",
    "from src.models.p4_allcnn import P4AllCNNC\n",
    "from src.models.fA_p4_allcnn import fA_P4AllCNNC\n",
    "from src.models.p4m_allcnn import  P4MAllCNNC\n",
    "from src.models.fA_p4m_allcnn import fA_P4MAllCNNC\n",
    "from src.models.p4m_resnet import  P4MResNet\n",
    "from src.models.fA_p4_resnet import fA_P4MResNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff987c1c-1bd1-43ff-9590-68b85acaaf0b",
   "metadata": {},
   "source": [
    "## CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8db10c-d12b-45c5-929e-80f5d4d0ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../data\"\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "transform_test = transforms.ToTensor()\n",
    "\n",
    "full_train = datasets.CIFAR10(root=root_path, train=True, download=True, transform=transform_train)\n",
    "\n",
    "train_size = int(0.9 * len(full_train))\n",
    "val_size = len(full_train) - train_size\n",
    "train_set, val_set = random_split(full_train, [train_size, val_size])\n",
    "test_set = datasets.CIFAR10(root=root_path, train=False, download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bb54b7e-50ea-4216-8f2e-c80bad071194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "img, label = full_train[0]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8241b-b5a8-479f-bb9e-29d536497c74",
   "metadata": {},
   "source": [
    "## Table generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eec0884-5ebf-44a7-b0e7-dcf1f71216db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training models...\n",
      "Training model : mlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [01:01<00:00, 30.57s/it, Train Acc=0.2501, Val Acc=0.2482, Train Loss=1.9953, Val Loss=1.9770]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model : p4_allcnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/2 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 135\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Train models on dataset\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining models...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Evaluate models on dataset\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting models...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/uni/msc/q4/frmdl/fundamental-research-project/experiments/../src/trainer/trainer.py:54\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, num_epochs, train_loader, val_loader)\u001b[39m\n\u001b[32m     52\u001b[39m outputs = model(inputs)\n\u001b[32m     53\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterions[i](outputs, targets)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizers[i].step()\n\u001b[32m     56\u001b[39m total_loss += loss.item() * inputs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Init training variables\n",
    "num_epochs = 2 #10 \n",
    "num_iterations = 1#3\n",
    "log_dir = \"../logs\"\n",
    "accuracies = {\n",
    "    \"mlp\": [],\n",
    "    \"p4_allcnn\": [],\n",
    "    \"fA_p4_allcnn\": [],\n",
    "    \"p4m_allcnn\": [],\n",
    "    \"fA_p4m_allcnn\": [],\n",
    "    \"p4m_resnet\": [],\n",
    "    \"fA_p4_resnet\": []\n",
    "}\n",
    "\n",
    "# ----- Init model paramerters -----\n",
    "\n",
    "## MLP parameters:\n",
    "input_size = 3 * 32 * 32\n",
    "output_size = 10\n",
    "hidden_sizes = [128, 64]\n",
    "\n",
    "## ---------------------------------\n",
    "\n",
    "for it in range(num_iterations):\n",
    "\n",
    "    # ----- Models initialization -----\n",
    "    mlp_model = MLP(\n",
    "        input_size = input_size,\n",
    "        hidden_sizes = hidden_sizes,\n",
    "        output_size = output_size\n",
    "    )\n",
    "\n",
    "    p4_allcnn_model = P4AllCNNC()\n",
    "    fA_p4_allcnn_model= fA_P4AllCNNC()\n",
    "    p4m_allcnn_model = P4MAllCNNC()\n",
    "    fA_p4m_allcnn_model = fA_P4MAllCNNC()\n",
    "\n",
    "    p4m_resnet_model = P4MResNet()\n",
    "    fA_p4_resnet_model= fA_P4MResNet()\n",
    "\n",
    "\n",
    "    # ---------------------------------\n",
    "\n",
    "    # ----- Other parameter initialization -----\n",
    "    mlp_criterion = nn.CrossEntropyLoss()\n",
    "    mlp_optimizer = optim.SGD(mlp_model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    mlp_scheduler = optim.lr_scheduler.MultiStepLR(mlp_optimizer, milestones=[200, 250, 300], gamma=0.1)\n",
    "\n",
    "    # P4AllCNNC\n",
    "    p4_allcnn_criterion = nn.CrossEntropyLoss()\n",
    "    p4_allcnn_optimizer = optim.SGD(p4_allcnn_model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    p4_allcnn_scheduler = optim.lr_scheduler.MultiStepLR(p4_allcnn_optimizer, milestones=[200, 250, 300], gamma=0.1)\n",
    "\n",
    "    # fA_P4AllCNNC\n",
    "    fA_p4_allcnn_criterion = nn.CrossEntropyLoss()\n",
    "    fA_p4_allcnn_optimizer = optim.SGD(fA_p4_allcnn_model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    fA_p4_allcnn_scheduler = optim.lr_scheduler.MultiStepLR(fA_p4_allcnn_optimizer, milestones=[200, 250, 300], gamma=0.1)\n",
    "\n",
    "    # P4MAllCNNC\n",
    "    p4m_allcnn_criterion = nn.CrossEntropyLoss()\n",
    "    p4m_allcnn_optimizer = optim.SGD(p4m_allcnn_model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    p4m_allcnn_scheduler = optim.lr_scheduler.MultiStepLR(p4m_allcnn_optimizer, milestones=[200, 250, 300], gamma=0.1)\n",
    "\n",
    "    # fA_P4MAllCNNC\n",
    "    fA_p4m_allcnn_criterion = nn.CrossEntropyLoss()\n",
    "    fA_p4m_allcnn_optimizer = optim.SGD(fA_p4m_allcnn_model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    fA_p4m_allcnn_scheduler = optim.lr_scheduler.MultiStepLR(fA_p4m_allcnn_optimizer, milestones=[200, 250, 300], gamma=0.1)\n",
    "\n",
    "    # P4MResNet\n",
    "    p4m_resnet_criterion = nn.CrossEntropyLoss()\n",
    "    p4m_resnet_optimizer = optim.SGD(p4m_resnet_model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    p4m_resnet_scheduler = optim.lr_scheduler.MultiStepLR(p4m_resnet_optimizer, milestones=[200, 250, 300], gamma=0.1)\n",
    "\n",
    "    # fA_P4MResNet\n",
    "    fA_p4_resnet_criterion = nn.CrossEntropyLoss()\n",
    "    fA_p4_resnet_optimizer = optim.SGD(fA_p4_resnet_model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    fA_p4_resnet_scheduler = optim.lr_scheduler.MultiStepLR(fA_p4_resnet_optimizer, milestones=[200, 250, 300], gamma=0.1)\n",
    "\n",
    "\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # Generate lists\n",
    "    models = {\n",
    "    \"mlp\": mlp_model,\n",
    "    \"p4_allcnn\": p4_allcnn_model,\n",
    "    \"fA_p4_allcnn\": fA_p4_allcnn_model,\n",
    "    \"p4m_allcnn\": p4m_allcnn_model,\n",
    "    \"fA_p4m_allcnn\": fA_p4m_allcnn_model,\n",
    "    \"p4m_resnet\": p4m_resnet_model,\n",
    "    \"fA_p4_resnet\": fA_p4_resnet_model\n",
    "    }\n",
    "\n",
    "    criterions = [\n",
    "        mlp_criterion,\n",
    "        p4_allcnn_criterion,\n",
    "        fA_p4_allcnn_criterion,\n",
    "        p4m_allcnn_criterion,\n",
    "        fA_p4m_allcnn_criterion,\n",
    "        p4m_resnet_criterion,\n",
    "        fA_p4_resnet_criterion\n",
    "    ]\n",
    "\n",
    "    optimizers = [\n",
    "        mlp_optimizer,\n",
    "        p4_allcnn_optimizer,\n",
    "        fA_p4_allcnn_optimizer,\n",
    "        p4m_allcnn_optimizer,\n",
    "        fA_p4m_allcnn_optimizer,\n",
    "        p4m_resnet_optimizer,\n",
    "        fA_p4_resnet_optimizer\n",
    "    ]\n",
    "\n",
    "    schedulers = [\n",
    "        mlp_scheduler,\n",
    "        p4_allcnn_scheduler,\n",
    "        fA_p4_allcnn_scheduler,\n",
    "        p4m_allcnn_scheduler,\n",
    "        fA_p4m_allcnn_scheduler,\n",
    "        p4m_resnet_scheduler,\n",
    "        fA_p4_resnet_scheduler\n",
    "    ]\n",
    "\n",
    "    # -------------------------------------------\n",
    "\n",
    "    trainer = Trainer(\n",
    "        models = models,\n",
    "        optimizers = optimizers,\n",
    "        criterions = criterions,\n",
    "        schedulers = schedulers,\n",
    "        log_dir = log_dir\n",
    "    )\n",
    "\n",
    "    # Train models on dataset\n",
    "    print(\"Training models...\")\n",
    "    trainer.train(\n",
    "        num_epochs = num_epochs,\n",
    "        train_loader = train_loader,\n",
    "        val_loader = val_loader,\n",
    "    )\n",
    "\n",
    "    # Evaluate models on dataset\n",
    "    print(\"Testing models...\")\n",
    "    test_accuracies = trainer.evaluate(test_loader = test_loader)\n",
    "\n",
    "    for model_name, acc in test_accuracies.items():\n",
    "        accuracies[model_name].append(acc)\n",
    "\n",
    "# get final statistics\n",
    "final_stats = {\n",
    "    model_name: {\n",
    "        \"% Test error\" : (1 - float(np.mean(values))) * 100,\n",
    "        \"% std\" : float(np.std(values)) * 100,\n",
    "        \"Num Parameters\" : sum(p.numel() for p in models[model_name].parameters()),\n",
    "    }\n",
    "    for model_name, values in accuracies.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2827a4-7a77-4f14-b1e4-f9eb7f10fd80",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dfa9492-1e4e-4e19-ad0f-7a384fa167f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model Accuracy Summary in CIFAR10\n",
      "\n",
      "‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï\n",
      "‚îÇ     ‚îÇ   % Test error ‚îÇ   % std ‚îÇ   Num Parameters ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ mlp ‚îÇ          74.52 ‚îÇ    2.38 ‚îÇ           402250 ‚îÇ\n",
      "‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(final_stats, orient='index')\n",
    "df = df.round(2)\n",
    "print(\"üìä Model Accuracy Summary in CIFAR10\\n\")\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"fancy_grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
